2024-01-30 21:30:34 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-30 21:30:34 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-30 21:30:35 | ERROR | stderr | Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                             | 0/2 [00:00<?, ?it/s]
2024-01-30 21:30:46 | ERROR | stderr | Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                          | 1/2 [00:10<00:10, 10.20s/it]
2024-01-30 21:30:49 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.37s/it]
2024-01-30 21:30:49 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.95s/it]
2024-01-30 21:30:49 | ERROR | stderr | 
2024-01-30 21:30:49 | ERROR | stderr | Traceback (most recent call last):
2024-01-30 21:30:49 | ERROR | stderr |   File "/home/dh/anaconda3/envs/video_chatgpt/lib/python3.8/site-packages/transformers/image_processing_utils.py", line 268, in get_image_processor_dict
2024-01-30 21:30:49 | ERROR | stderr |     resolved_image_processor_file = cached_file(
2024-01-30 21:30:49 | ERROR | stderr |   File "/home/dh/anaconda3/envs/video_chatgpt/lib/python3.8/site-packages/transformers/utils/hub.py", line 409, in cached_file
2024-01-30 21:30:49 | ERROR | stderr |     resolved_file = hf_hub_download(
2024-01-30 21:30:49 | ERROR | stderr |   File "/home/dh/anaconda3/envs/video_chatgpt/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 110, in _inner_fn
2024-01-30 21:30:49 | ERROR | stderr |     validate_repo_id(arg_value)
2024-01-30 21:30:49 | ERROR | stderr |   File "/home/dh/anaconda3/envs/video_chatgpt/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 158, in validate_repo_id
2024-01-30 21:30:49 | ERROR | stderr |     raise HFValidationError(
2024-01-30 21:30:49 | ERROR | stderr | huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/disk1/hub/models--openai--clip-vit-large-patch14/snapshots/8d052a0f05efbaefbc9e8786ba291cfdf93e5bff'. Use `repo_type` argument if needed.
2024-01-30 21:30:49 | ERROR | stderr | 
2024-01-30 21:30:49 | ERROR | stderr | During handling of the above exception, another exception occurred:
2024-01-30 21:30:49 | ERROR | stderr | 
2024-01-30 21:30:49 | ERROR | stderr | Traceback (most recent call last):
2024-01-30 21:30:49 | ERROR | stderr |   File "inference_CUVA.py", line 136, in <module>
2024-01-30 21:30:49 | ERROR | stderr |     initialize_model(args.model_name, args.projection_path, args.vision_tower_path)
2024-01-30 21:30:49 | ERROR | stderr |   File "/home/dh/pythonProject/AnomalyDataset/Video-ChatGPT/video_chatgpt/eval/model_utils.py", line 271, in initialize_model
2024-01-30 21:30:49 | ERROR | stderr |     image_processor = CLIPImageProcessor.from_pretrained(vision_tower_path, torch_dtype=torch.float16)
2024-01-30 21:30:49 | ERROR | stderr |   File "/home/dh/anaconda3/envs/video_chatgpt/lib/python3.8/site-packages/transformers/image_processing_utils.py", line 164, in from_pretrained
2024-01-30 21:30:49 | ERROR | stderr |     image_processor_dict, kwargs = cls.get_image_processor_dict(pretrained_model_name_or_path, **kwargs)
2024-01-30 21:30:49 | ERROR | stderr |   File "/home/dh/anaconda3/envs/video_chatgpt/lib/python3.8/site-packages/transformers/image_processing_utils.py", line 287, in get_image_processor_dict
2024-01-30 21:30:49 | ERROR | stderr |     raise EnvironmentError(
2024-01-30 21:30:49 | ERROR | stderr | OSError: Can't load image processor for '/home/disk1/hub/models--openai--clip-vit-large-patch14/snapshots/8d052a0f05efbaefbc9e8786ba291cfdf93e5bff'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/disk1/hub/models--openai--clip-vit-large-patch14/snapshots/8d052a0f05efbaefbc9e8786ba291cfdf93e5bff' is the correct path to a directory containing a preprocessor_config.json file
2024-01-30 21:31:45 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-30 21:31:45 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-30 21:31:46 | ERROR | stderr | Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                             | 0/2 [00:00<?, ?it/s]
2024-01-30 21:31:55 | ERROR | stderr | Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                          | 1/2 [00:08<00:08,  8.80s/it]
2024-01-30 21:31:58 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  5.51s/it]
2024-01-30 21:31:58 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.00s/it]
2024-01-30 21:31:58 | ERROR | stderr | 
2024-01-30 21:32:15 | INFO | stdout | Loading weights from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-30 21:32:16 | INFO | stdout | Weights loaded from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-30 21:32:31 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.final_layer_norm.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'visual_projection.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'logit_scale', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-30 21:32:31 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.final_layer_norm.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'visual_projection.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'logit_scale', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-30 21:32:31 | INFO | stdout | Initialization Finished
2024-01-30 21:32:31 | ERROR | stderr | Traceback (most recent call last):
2024-01-30 21:32:31 | ERROR | stderr |   File "CUVA_demo.py", line 174, in <module>
2024-01-30 21:32:31 | ERROR | stderr |     for video_name in os.listdir(video_dir[0:2]):
2024-01-30 21:32:31 | ERROR | stderr | FileNotFoundError: [Errno 2] No such file or directory: '/h'
2024-01-30 21:33:13 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-30 21:33:13 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-30 21:33:14 | ERROR | stderr | Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                             | 0/2 [00:00<?, ?it/s]
2024-01-30 21:33:22 | ERROR | stderr | Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                          | 1/2 [00:08<00:08,  8.80s/it]
2024-01-30 21:33:26 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  5.51s/it]
2024-01-30 21:33:26 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.00s/it]
2024-01-30 21:33:26 | ERROR | stderr | 
2024-01-30 21:33:42 | INFO | stdout | Loading weights from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-30 21:33:43 | INFO | stdout | Weights loaded from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-30 21:33:56 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_projection.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'visual_projection.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'logit_scale', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.bias']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-30 21:33:56 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_projection.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'visual_projection.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'logit_scale', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.bias']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-30 21:33:57 | INFO | stdout | Initialization Finished
2024-01-30 21:33:57 | ERROR | stderr | Traceback (most recent call last):
2024-01-30 21:33:57 | ERROR | stderr |   File "CUVA_demo.py", line 174, in <module>
2024-01-30 21:33:57 | ERROR | stderr |     for video_name in tqdm(os.listdir(video_dir)[0,2]):
2024-01-30 21:33:57 | ERROR | stderr | TypeError: list indices must be integers or slices, not tuple
2024-01-30 21:34:20 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-30 21:34:20 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-30 21:34:21 | ERROR | stderr | Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                             | 0/2 [00:00<?, ?it/s]
2024-01-30 21:34:30 | ERROR | stderr | Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                          | 1/2 [00:08<00:08,  8.79s/it]
2024-01-30 21:34:33 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.48s/it]
2024-01-30 21:34:33 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.98s/it]
2024-01-30 21:34:33 | ERROR | stderr | 
2024-01-30 21:34:50 | INFO | stdout | Loading weights from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-30 21:34:51 | INFO | stdout | Weights loaded from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-30 21:35:02 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'visual_projection.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'logit_scale', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.weight']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-30 21:35:02 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'visual_projection.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'logit_scale', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.weight']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-30 21:35:03 | INFO | stdout | Initialization Finished
2024-01-30 21:35:03 | ERROR | stderr |   0%|                                                                                                                                                                                                                                                                                        | 0/2 [00:00<?, ?it/s]
2024-01-30 21:35:13 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 21:35:13 | ERROR | stderr | /home/dh/anaconda3/envs/video_chatgpt/lib/python3.8/site-packages/transformers/generation/utils.py:1211: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
2024-01-30 21:35:13 | ERROR | stderr |   warnings.warn(
2024-01-30 21:35:25 | INFO | gradio_web_server | In the video, a man is seen driving a car and talking on his phone. This is an unusual behavior as it is generally not safe to use a phone while driving. The video also shows a man walking in the street and talking on his phone. This is another unusual behavior as it is not safe to walk in the street while talking on a phone. The video also shows a man driving a car and talking on his phone. This is a dangerous behavior as it can lead to accidents and endanger the lives of others on the road.
2024-01-30 21:35:25 | ERROR | stderr |  50%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                        | 1/2 [00:22<00:22, 22.12s/it]
2024-01-30 21:35:28 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 21:35:44 | INFO | gradio_web_server | In the video, a man is seen walking across the grass in the middle of the circle, while a woman is sitting on the grass. This is an unusual event because people usually do not walk across the grass in the middle of a circle. Additionally, there is a car driving around the circle, which is also an unusual sight. The presence of a fire hydrant in the scene is also noteworthy, as it is not a common object to be found in the middle of a grassy circle. The video captures a moment of a man walking across the grass in the middle of the circle, a woman sitting on the grass, and a car driving around the circle, all of which are atypical occurrences in a typical grassy circle.
2024-01-30 21:35:44 | ERROR | stderr | 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:41<00:00, 20.43s/it]
2024-01-30 21:35:44 | ERROR | stderr | 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:41<00:00, 20.68s/it]
2024-01-30 21:35:44 | ERROR | stderr | 
2024-01-30 22:14:21 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-30 22:14:21 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-30 22:14:22 | ERROR | stderr | Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                             | 0/2 [00:00<?, ?it/s]
2024-01-30 22:14:31 | ERROR | stderr | Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                          | 1/2 [00:08<00:08,  8.77s/it]
2024-01-30 22:14:34 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.50s/it]
2024-01-30 22:14:34 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.99s/it]
2024-01-30 22:14:34 | ERROR | stderr | 
2024-01-30 22:14:50 | INFO | stdout | Loading weights from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-30 22:14:51 | INFO | stdout | Weights loaded from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-30 22:15:03 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'visual_projection.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'logit_scale', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-30 22:15:03 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'visual_projection.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'logit_scale', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-30 22:15:03 | INFO | stdout | Initialization Finished
2024-01-30 22:15:03 | ERROR | stderr |   0%|                                                                                                                                                                                                                                                                                       | 0/20 [00:00<?, ?it/s]
2024-01-30 22:15:12 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:15:12 | ERROR | stderr | /home/dh/anaconda3/envs/video_chatgpt/lib/python3.8/site-packages/transformers/generation/utils.py:1211: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
2024-01-30 22:15:12 | ERROR | stderr |   warnings.warn(
2024-01-30 22:15:31 | INFO | gradio_web_server | In the video, a man is seen driving a car and talking on his phone. This is an unusual behavior as it is generally not safe to use a phone while driving. The video also shows a man walking in the street and talking on his phone. This is another unusual behavior as it is not safe to walk in the street while talking on a phone. The video also shows a man walking in the street and talking on his phone while holding a cup. This is an even more unusual behavior as it is not safe to walk in the street while holding a cup. The video also shows a man walking in the street and talking on his phone while holding a cup and a bag. This is the most unusual behavior as it is not safe to walk in the street while holding a cup, a bag, and talking on a phone.
2024-01-30 22:15:31 | ERROR | stderr |   5%|█████████████▌                                                                                                                                                                                                                                                                 | 1/20 [00:27<08:36, 27.17s/it]
2024-01-30 22:15:34 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:15:48 | INFO | gradio_web_server | In the video, a man is seen kneeling on the grass in the middle of the circle. This is an unusual event as people usually do not kneel in the middle of a grassy circle. The man is also holding a cell phone, which is not a common activity in such a setting. Additionally, there are cars driving around the circle, and a person is seen walking around the circle. The presence of a truck and a car in the circle is also unusual, as it is not a typical location for vehicles to be parked or moving. The combination of these events creates an interesting and unusual scene in the video.
2024-01-30 22:15:48 | ERROR | stderr |  10%|███████████████████████████                                                                                                                                                                                                                                                    | 2/20 [00:44<06:24, 21.36s/it]
2024-01-30 22:15:53 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:19:22 | INFO | gradio_web_server | In the video, a man is seen driving a school bus and turning on the red lights. The bus is then seen driving down the street, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out of the bus. The bus then drives away, and a man is seen getting out of the bus. The bus then stops at a red light, and a man is seen getting out
2024-01-30 22:19:22 | ERROR | stderr |  15%|████████████████████████████████████████▌                                                                                                                                                                                                                                     | 3/20 [04:18<31:01, 109.50s/it]
2024-01-30 22:19:25 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:19:31 | INFO | gradio_web_server | In the video, a man is seen driving a car and then getting out of it. He is then seen walking towards a woman who is sitting in a car. The man is seen talking to the woman, and they both get into the car together. The car then drives away.
2024-01-30 22:19:31 | ERROR | stderr |  20%|██████████████████████████████████████████████████████▏                                                                                                                                                                                                                        | 4/20 [04:28<18:38, 69.90s/it]
2024-01-30 22:19:45 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:20:13 | INFO | gradio_web_server | In the video, there is a scene where a person is seen driving a car down a street at night. The car is seen driving down the street and then suddenly stops. The person gets out of the car and starts walking away. This is an unusual event because it is not common for a driver to stop their car and walk away from it.

Another scene shows a person walking down the street at night. The person is seen walking down the street and then suddenly stops. This is also an unusual event because it is not common for a pedestrian to stop walking and stand still in the middle of the street.

In the final scene, a person is seen driving a car down the street at night. The car is seen driving down the street and then suddenly stops. The person gets out of the car and starts walking away. This is an unusual event because it is not common for a driver to stop their car and walk away from it.

In summary, the video shows three unusual events: a driver stopping their car and walking away, a pedestrian stopping and standing still in the middle of the street, and a person driving a car and stopping. These events deviate from the norm and are worth noting.
2024-01-30 22:20:13 | ERROR | stderr |  25%|███████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                           | 5/20 [05:09<14:55, 59.71s/it]
2024-01-30 22:20:17 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:20:20 | INFO | gradio_web_server | In the video, a man is seen driving a car and suddenly stops. The car is then seen driving away from the scene.
2024-01-30 22:20:20 | ERROR | stderr |  30%|█████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                             | 6/20 [05:16<09:46, 41.86s/it]
2024-01-30 22:20:24 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:20:31 | INFO | gradio_web_server | In the video, a man is seen driving a car and talking on his phone. He is then seen driving the car into a parking lot and talking on his phone. The car then drives into a parking lot and stops. The man gets out of the car and starts talking on his phone. The car then drives away.
2024-01-30 22:20:31 | ERROR | stderr |  35%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                | 7/20 [05:27<06:51, 31.66s/it]
2024-01-30 22:20:34 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:21:02 | INFO | gradio_web_server | In the video, a man is seen holding a gun and pointing it at another man. This is an unusual event as guns are not typically used in everyday situations. The man holding the gun is likely a criminal or a person involved in a dangerous situation.

Another scene shows a man holding a knife and stabbing another man. This is also an unusual event as knife fights are not common in everyday life. The man holding the knife is likely a criminal or a person involved in a dangerous situation.

In another scene, a man is seen holding a bottle and throwing it at another man. This is an unusual event as throwing bottles is not a typical behavior in everyday life. The man throwing the bottle is likely a criminal or a person involved in a dangerous situation.

In the final scene, a man is seen holding a gun and pointing it at another man. This is an unusual event as guns are not typically used in everyday situations. The man holding the gun is likely a criminal or a person involved in a dangerous situation.

These scenes showcase various unusual events that deviate from the norm, highlighting the importance of anomaly detection in video analysis.
2024-01-30 22:21:02 | ERROR | stderr |  40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                  | 8/20 [05:59<06:19, 31.62s/it]
2024-01-30 22:21:04 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:24:36 | INFO | gradio_web_server | In the video, a man is seen driving a car and then getting out of it. He is then seen walking away from the car and talking to a woman. The car is then seen driving away from the scene.

The video also shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video also shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a car and talking to a woman. The car is then seen driving away from the scene.

The video shows a man walking away from a
2024-01-30 22:24:36 | ERROR | stderr |  45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                     | 9/20 [09:33<16:14, 88.63s/it]
2024-01-30 22:24:39 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:24:51 | INFO | gradio_web_server | In the video, a man is seen throwing a punch at another man, who is then seen holding a gun. This is an unusual event as it is not common to see someone holding a gun in a public setting. The man holding the gun is likely a criminal or a person with a history of violence. The man throwing the punch is likely a bystander or a friend of the person holding the gun. The presence of a gun in a public setting raises concerns about safety and the potential for escalation of violence.
2024-01-30 22:24:51 | ERROR | stderr |  50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                       | 10/20 [09:47<10:58, 65.81s/it]
2024-01-30 22:24:55 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:25:01 | INFO | gradio_web_server | In the video, a man is seen driving a car and suddenly swerving to avoid a person who is standing in the middle of the road. The car then crashes into a guardrail and comes to a stop. The man gets out of the car and walks away.
2024-01-30 22:25:01 | ERROR | stderr |  55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                         | 11/20 [09:57<07:17, 48.62s/it]
2024-01-30 22:25:03 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:28:42 | INFO | gradio_web_server | In the video, there is a man riding a motorcycle and a car driving down the road. The man on the motorcycle is wearing a helmet and a jacket. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving down the road and the man on the motorcycle is following it. The car is driving
2024-01-30 22:28:42 | ERROR | stderr |  60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                           | 12/20 [13:39<13:30, 101.25s/it]
2024-01-30 22:28:45 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:29:15 | INFO | gradio_web_server | In the video, a man is seen walking down a street and suddenly a car appears out of nowhere and hits him. This is an unusual event as cars are not supposed to appear out of nowhere and hit pedestrians. The man is then seen lying on the ground, and the car drives away. This is an example of a traffic accident involving a pedestrian.

Next, a man is seen walking down a street and suddenly a person appears out of nowhere and hits him with a baseball bat. This is another unusual event as baseball bats are not typically used for such purposes. The man is then seen lying on the ground, and the person who hit him with the baseball bat drives away. This is an example of a violent crime involving a baseball bat.

In the final scene, a man is seen walking down a street and suddenly a person appears out of nowhere and hits him with a bottle. This is another unusual event as bottles are not typically used for such purposes. The man is then seen lying on the ground, and the person who hit him with the bottle drives away. This is an example of a violent crime involving a bottle.

In summary, the video contains three unusual events involving pedestrians being hit by objects, which are not typical in everyday life.
2024-01-30 22:29:15 | ERROR | stderr |  65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                              | 13/20 [14:11<09:22, 80.42s/it]
2024-01-30 22:29:22 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:29:35 | INFO | gradio_web_server | In the video, a man is seen driving a car and talking on his phone. This is an unusual behavior as it is generally not safe to use a phone while driving. Additionally, the car is seen driving down a street with a person in the back seat. This is also unusual as it is not common for a person to sit in the back seat of a car while it is in motion. The video also shows a person walking in the street, which is not a safe practice. Overall, the video captures several unusual events that deviate from the norm, making it a valuable resource for anomaly detection.
2024-01-30 22:29:35 | ERROR | stderr |  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                 | 14/20 [14:32<06:14, 62.33s/it]
2024-01-30 22:29:36 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:29:45 | INFO | gradio_web_server | In the video, a man is seen driving a car down the road, and the car suddenly stops. A police officer is seen pulling the driver over, and the driver is seen getting out of the car. The police officer is seen talking to the driver, and the driver is seen getting back into the car. The police officer is then seen driving away from the scene.
2024-01-30 22:29:45 | ERROR | stderr |  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                   | 15/20 [14:41<03:51, 46.33s/it]
2024-01-30 22:29:50 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:33:30 | INFO | gradio_web_server | In the video, there is a man who is seen driving a car and then getting out of it. He is then seen walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to a woman. The video also shows a man walking down the street and talking to
2024-01-30 22:33:30 | ERROR | stderr |  80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                     | 16/20 [18:26<06:40, 100.13s/it]
2024-01-30 22:33:38 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:33:45 | INFO | gradio_web_server | In the video, a man is seen driving a motorcycle and a car. The car is seen driving in front of the motorcycle, and the motorcycle is seen driving behind the car. This is an unusual event as it is not common for a motorcycle to drive behind a car.
2024-01-30 22:33:45 | ERROR | stderr |  85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                        | 17/20 [18:41<03:43, 74.51s/it]
2024-01-30 22:33:46 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:34:19 | INFO | gradio_web_server | In the video, a car is seen driving down the street and suddenly stops. A man is seen getting out of the car and walking towards the camera. The car then starts moving again, and the man is seen walking away from the camera. This is an unusual event as it is not common for a person to get out of a car and walk towards the camera while the car is in motion.

Next, a car is seen driving down the street and suddenly stops. A man is seen getting out of the car and walking towards the camera. The car then starts moving again, and the man is seen walking away from the camera. This is another unusual event as it is not common for a person to get out of a car and walk towards the camera while the car is in motion.

Finally, a car is seen driving down the street and suddenly stops. A man is seen getting out of the car and walking towards the camera. The car then starts moving again, and the man is seen walking away from the camera. This is the third unusual event in the video, as it is not common for a person to get out of a car and walk towards the camera while the car is in motion.

In summary, the video shows three unusual events where a person gets out of a car and walks towards the camera while the car is in motion. This is not a common occurrence and adds an element of intrigue to the video.
2024-01-30 22:34:19 | ERROR | stderr |  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                           | 18/20 [19:15<02:04, 62.28s/it]
2024-01-30 22:34:20 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:37:57 | INFO | gradio_web_server | The video shows a car driving down the street and a man in a white shirt walking in the middle of the street. The car stops and the man walks towards the car. The car then starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to walk in the middle of the street. The car stops again and the man walks towards the car. The car starts moving again and the man continues to
2024-01-30 22:37:57 | ERROR | stderr |  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 19/20 [22:53<01:49, 109.12s/it]
2024-01-30 22:37:58 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:41:32 | INFO | gradio_web_server | In the video, there is a man driving a car and a woman walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on the sidewalk. The man is driving the car and the woman is walking on
2024-01-30 22:41:32 | ERROR | stderr | 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [26:28<00:00, 140.90s/it]
2024-01-30 22:41:32 | ERROR | stderr | 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [26:28<00:00, 79.42s/it]
2024-01-30 22:41:32 | ERROR | stderr | 
2024-01-30 22:49:11 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-30 22:49:11 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-30 22:49:12 | ERROR | stderr | Loading checkpoint shards:   0%|                                                                                                                     | 0/2 [00:00<?, ?it/s]
2024-01-30 22:49:21 | ERROR | stderr | Loading checkpoint shards:  50%|██████████████████████████████████████████████████████▌                                                      | 1/2 [00:08<00:08,  8.66s/it]
2024-01-30 22:49:24 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  5.72s/it]
2024-01-30 22:49:24 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.16s/it]
2024-01-30 22:49:24 | ERROR | stderr | 
2024-01-30 22:49:42 | INFO | stdout | Loading weights from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-30 22:49:42 | INFO | stdout | Weights loaded from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-30 22:49:56 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_projection.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'visual_projection.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'logit_scale', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-30 22:49:56 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_projection.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'visual_projection.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'logit_scale', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-30 22:49:57 | INFO | stdout | Initialization Finished
2024-01-30 22:49:57 | ERROR | stderr |   0%|                                                                                                                                               | 0/20 [00:00<?, ?it/s]
2024-01-30 22:50:05 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:50:05 | ERROR | stderr | /home/dh/anaconda3/envs/video_chatgpt/lib/python3.8/site-packages/transformers/generation/utils.py:1211: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
2024-01-30 22:50:05 | ERROR | stderr |   warnings.warn(
2024-01-30 22:50:25 | INFO | gradio_web_server | The anomalies in the video are likely caused by a combination of factors, including human error, mechanical failure, or external factors. In the video, we see a car driving down the road and suddenly swerving to avoid a person who is standing in the middle of the road. This could be due to the driver not paying attention to the road or the person, or the person might have been playing a prank or attempting to cross the road without paying attention to the traffic. Additionally, the car could have experienced a mechanical failure, such as a flat tire or brake failure, which led to the driver swerving to avoid the person. In any case, the video highlights the importance of being vigilant and attentive while driving, as well as the need for proper road safety measures and infrastructure to prevent such incidents from occurring.
2024-01-30 22:50:25 | ERROR | stderr |   5%|██████▊                                                                                                                                | 1/20 [00:28<08:59, 28.42s/it]
2024-01-30 22:50:28 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:50:50 | INFO | gradio_web_server | The anomalies in the video, such as the man walking on the grass and the woman sitting on the ground, are likely due to the presence of a large group of people in the area. The video shows a busy street with cars and people walking around, which could have led to the man walking on the grass and the woman sitting on the ground. The presence of a large group of people in the area might have caused the man to walk on the grass to avoid the crowd, while the woman might have chosen to sit on the ground to rest or take a break from the busy street. The video also shows a car driving down the street, which could have contributed to the overall congestion and the need for the man to walk on the grass. The presence of a large group of people and a car in the area could have led to the unusual behavior of the man walking on the grass and the woman sitting on the ground.
2024-01-30 22:50:50 | ERROR | stderr |  10%|█████████████▌                                                                                                                         | 2/20 [00:53<07:52, 26.24s/it]
2024-01-30 22:50:55 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:51:20 | INFO | gradio_web_server | The anomalies in the video are likely due to a combination of factors, including human error, technical issues, and external factors. In the video, we see a school bus running red lights, which is a clear violation of traffic rules. This could be due to the bus driver's negligence or a technical issue with the bus's braking system. Additionally, the presence of a man in a red shirt and a woman in a white shirt in the video suggests that there might be a person or a group of people responsible for the bus's operation. It is possible that they are responsible for the bus's maintenance or operation, and they might have failed to notice or address the issue with the bus's braking system. Furthermore, external factors such as weather conditions, road conditions, or even the bus driver's fatigue could also contribute to the bus running red lights. To prevent such incidents in the future, it is essential to ensure proper maintenance of the bus, regular inspections, and adequate training for the bus driver and other staff members responsible for the bus's operation.
2024-01-30 22:51:20 | ERROR | stderr |  15%|████████████████████▎                                                                                                                  | 3/20 [01:23<07:57, 28.10s/it]
2024-01-30 22:51:23 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:51:42 | INFO | gradio_web_server | The anomalies in the video are likely due to the presence of a car accident, which is captured in the video. The car accident is the primary reason behind the abnormal events in the video. The video shows a car accident involving a white car and a red car, which is captured in a close-up shot. The accident is followed by a man in a white shirt and a woman in a red shirt, who are seen talking to each other. The video also shows a man in a white shirt talking to a woman in a red shirt, who is seen talking to a man in a white shirt. The video ends with a close-up shot of a man in a white shirt talking to a woman in a red shirt. The presence of the car accident and the people involved in the conversation are the primary reasons behind the abnormal events in the video.
2024-01-30 22:51:42 | ERROR | stderr |  20%|███████████████████████████                                                                                                            | 4/20 [01:45<06:50, 25.63s/it]
2024-01-30 22:51:56 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:52:18 | INFO | gradio_web_server | The anomalies in the video are likely caused by a combination of factors, including the driver's behavior, the car's performance, and external factors such as weather conditions. In the video, the car is seen driving at high speeds, which could be a contributing factor to the abnormal events. Additionally, the driver may be under the influence of alcohol or drugs, which could lead to reckless driving and the appearance of anomalies. Furthermore, the car's performance may be affected by mechanical issues, such as a malfunctioning brake system or a faulty tire, which could cause the car to swerve or lose control. External factors, such as poor visibility due to rain or snow, could also contribute to the anomalies observed in the video. To determine the root causes of these anomalies, it is essential to analyze the video footage and consider the various factors that could have led to the car's behavior and the appearance of the abnormal events.
2024-01-30 22:52:18 | ERROR | stderr |  25%|█████████████████████████████████▊                                                                                                     | 5/20 [02:21<07:22, 29.48s/it]
2024-01-30 22:52:22 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:55:50 | INFO | gradio_web_server | The video shows a car accident involving a white car and a truck. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck. The video also shows a person in the car who is not wearing a seatbelt. The car is seen flipping over and crashing into the truck.
2024-01-30 22:55:50 | ERROR | stderr |  30%|████████████████████████████████████████▌                                                                                              | 6/20 [05:53<21:20, 91.45s/it]
2024-01-30 22:55:53 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:56:13 | INFO | gradio_web_server | The anomalies in the video are likely caused by a combination of factors, including the presence of a car accident, a fire, and a police officer talking to a man. The car accident could have led to the fire, which in turn could have caused the police officer to stop and talk to the man. The presence of these events in the video suggests that the area is experiencing a high level of activity, which could be due to a variety of reasons such as road construction, a nearby event, or a natural disaster. The police officer's actions in the video indicate that they are trying to manage the situation and ensure the safety of the people involved. The presence of the fire and the car accident also highlights the importance of proper road safety measures and the need for emergency services to be prepared to handle such incidents.
2024-01-30 22:56:13 | ERROR | stderr |  35%|███████████████████████████████████████████████▎                                                                                       | 7/20 [06:16<14:59, 69.19s/it]
2024-01-30 22:56:17 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:56:40 | INFO | gradio_web_server | The anomalies in the video are likely caused by the actions of the two men in the scene. One man is seen holding a gun and pointing it at the other man, who is lying on the ground. The other man is seen holding a knife and stabbing the first man. These actions are not typical in everyday life, and they are likely to be the result of a conflict or a disagreement between the two men. The presence of a gun and a knife in the scene suggests that the situation is potentially dangerous and could have escalated to a physical altercation. The video captures the moment when the situation has reached its peak, and the outcome is uncertain. The reasons behind the anomalies in the video are likely related to the conflict between the two men, which could be due to personal, financial, or emotional issues. The video serves as a reminder of the consequences of such conflicts and the importance of finding peaceful solutions to resolve disagreements.
2024-01-30 22:56:40 | ERROR | stderr |  40%|██████████████████████████████████████████████████████                                                                                 | 8/20 [06:43<11:06, 55.58s/it]
2024-01-30 22:56:41 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:57:13 | INFO | gradio_web_server | The anomalies in the video are likely caused by a combination of factors, including human error, equipment malfunction, and external factors. In the video, we see a car crashing into a traffic light, which could be attributed to a driver's mistake or a mechanical failure in the car. Additionally, the presence of a fire hydrant in the scene suggests that there might be a fire nearby, which could have led to the traffic light malfunctioning or being knocked down. Furthermore, the presence of a news reporter in the scene indicates that the incident is being covered by the media, which could have contributed to the traffic light malfunctioning or being knocked down.

In conclusion, the anomalies in the video are a result of a combination of factors, including human error, equipment malfunction, and external factors. The presence of a news reporter in the scene suggests that the incident is being covered by the media, which could have contributed to the traffic light malfunctioning or being knocked down. The car crashing into the traffic light also indicates a driver's mistake or a mechanical failure in the car. Overall, the video highlights the importance of proper road safety measures, effective communication between emergency services, and the need for regular maintenance of traffic infrastructure.
2024-01-30 22:57:13 | ERROR | stderr |  45%|████████████████████████████████████████████████████████████▊                                                                          | 9/20 [07:16<08:53, 48.54s/it]
2024-01-30 22:57:15 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:57:35 | INFO | gradio_web_server | The anomalies in the video are likely caused by the presence of a man in a white shirt who is seen in multiple scenes. In one scene, he is seen holding a gun and pointing it at a man, while in another scene, he is seen holding a knife and threatening another man. These actions are not consistent with the typical behavior of the other people in the video, who are seen engaging in more casual activities such as drinking and smoking. The presence of the man in the white shirt creates a sense of tension and unease, as his actions are not in line with the normal behavior of the other individuals in the video. This discrepancy in behavior may have led to the anomalies observed in the video, such as the man in the white shirt being the only one who is not smiling.
2024-01-30 22:57:35 | ERROR | stderr |  50%|███████████████████████████████████████████████████████████████████                                                                   | 10/20 [07:38<06:44, 40.44s/it]
2024-01-30 22:57:39 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:58:26 | INFO | gradio_web_server | The anomalies in the video are likely caused by a combination of factors, including human error, mechanical failure, or external factors. In the video, we see a car driving down the road and suddenly swerving to avoid a person who is standing in the middle of the road. This could be due to the driver not paying attention to the road or the person, or the person might have been playing a prank or attempting to cross the road without paying attention to the traffic. Additionally, the car might have experienced a mechanical failure, such as a flat tire or brake failure, which led to the driver losing control of the vehicle.

In another scene, we see a person standing in the middle of the road, which could be due to the person being intoxicated or experiencing a medical emergency. The driver might not have noticed the person in time to avoid a collision, or the person might have been playing a prank or attempting to cross the road without paying attention to the traffic.

In the final scene, we see a car driving down the road and suddenly swerving to avoid a person who is standing in the middle of the road. This could be due to the driver not paying attention to the road or the person, or the person might have been playing a prank or attempting to cross the road without paying attention to the traffic.

In conclusion, the anomalies in the video are likely caused by a combination of factors, including human error, mechanical failure, or external factors. The driver might not have noticed the person in time to avoid a collision, or the person might have been playing a prank or attempting to cross the road without paying attention to the traffic.
2024-01-30 22:58:26 | ERROR | stderr |  55%|█████████████████████████████████████████████████████████████████████████▋                                                            | 11/20 [08:28<06:31, 43.50s/it]
2024-01-30 22:58:28 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:58:43 | INFO | gradio_web_server | The anomalies in the video are likely caused by the presence of a person riding a motorcycle in the middle of the road. The person is seen riding the motorcycle in the middle of the road, which is not only dangerous but also against the rules of the road. This behavior can lead to accidents and endanger the lives of other road users. The video shows the person riding the motorcycle in the middle of the road, which is a clear indication of the person's reckless behavior. The presence of the person riding the motorcycle in the middle of the road is the primary cause of the anomalies in the video.
2024-01-30 22:58:43 | ERROR | stderr |  60%|████████████████████████████████████████████████████████████████████████████████▍                                                     | 12/20 [08:46<04:44, 35.56s/it]
2024-01-30 22:58:45 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:58:58 | INFO | gradio_web_server | The anomalies in the video are likely caused by the presence of a person with a gun. In the video, a man is seen holding a gun and pointing it at another person. This act of violence is not a typical occurrence in everyday life, and it is likely that the person holding the gun is a criminal or a person with a history of violence. The presence of the gun in the video is the primary reason for the abnormal events, and it is likely that the person holding the gun is responsible for the violence depicted in the video.
2024-01-30 22:58:58 | ERROR | stderr |  65%|███████████████████████████████████████████████████████████████████████████████████████                                               | 13/20 [09:01<03:25, 29.30s/it]
2024-01-30 22:59:05 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:59:22 | INFO | gradio_web_server | The anomalies in the video are likely caused by the driver's actions or the car's performance. The driver is seen driving the car at high speeds, which could be the reason for the car's sudden acceleration and the driver's inability to control it. Additionally, the car is seen swerving and making sharp turns, which could be due to the driver's inexperience or lack of proper training. The car's performance may also be affected by external factors such as road conditions, weather, or mechanical issues. To prevent such incidents in the future, it is essential for the driver to receive proper training, maintain the car regularly, and adhere to traffic rules and regulations.
2024-01-30 22:59:22 | ERROR | stderr |  70%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                        | 14/20 [09:25<02:45, 27.63s/it]
2024-01-30 22:59:23 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 22:59:37 | INFO | gradio_web_server | The anomalies in the video are likely caused by the driver's actions, which led to the wrong-way driver situation. The driver is seen driving the wrong way down the highway, which is a dangerous and illegal act. The presence of the police officer in the video suggests that the driver was stopped and the officer is trying to get the driver to stop. The driver's actions, such as driving the wrong way, are the primary cause of the anomalies in the video. The officer's presence and the driver's actions are the main factors contributing to the unusual events captured in the video.
2024-01-30 22:59:37 | ERROR | stderr |  75%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 15/20 [09:40<01:59, 23.87s/it]
2024-01-30 22:59:42 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 23:00:00 | INFO | gradio_web_server | The anomalies in the video are likely caused by a combination of factors, including human error, mechanical failure, or external factors. In the video, we see a car driving down the street and suddenly stopping in the middle of the road. This could be due to a driver losing control of the vehicle, a mechanical failure, or an external factor such as a sudden obstacle or a malfunctioning traffic light. Additionally, we see a person walking in the middle of the street, which could be due to a lack of awareness of the traffic rules or a desire to cross the street without using the designated crosswalks. In both cases, the anomalies are a result of human behavior or external factors that deviate from the norm, leading to the observed abnormalities in the video.
2024-01-30 23:00:00 | ERROR | stderr |  80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▏                          | 16/20 [10:03<01:34, 23.73s/it]
2024-01-30 23:00:09 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 23:00:45 | INFO | gradio_web_server | The anomalies in the video are likely caused by a combination of factors, including human error, mechanical failure, or external factors. In the video, we see a car driving on the wrong side of the road, which is an unusual behavior. This could be due to the driver being distracted or not paying attention to the road, or it could be a result of a mechanical failure in the car. Additionally, we see a car driving on the wrong side of the road and then turning around and driving back on the correct side. This could be due to the driver realizing their mistake and correcting it.

Another anomaly in the video is a car driving on the wrong side of the road and then turning around and driving back on the correct side. This could be due to the driver realizing their mistake and correcting it.

In the video, we also see a car driving on the wrong side of the road and then turning around and driving back on the correct side. This could be due to the driver realizing their mistake and correcting it.

In conclusion, the anomalies in the video are likely caused by a combination of factors, including human error, mechanical failure, or external factors. The drivers in the video may have been distracted, not paying attention to the road, or experiencing a mechanical failure in their vehicles. The presence of these anomalies highlights the importance of road safety and the need for drivers to remain vigilant and attentive to their surroundings.
2024-01-30 23:00:45 | ERROR | stderr |  85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                    | 17/20 [10:48<01:30, 30.11s/it]
2024-01-30 23:00:46 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 23:01:14 | INFO | gradio_web_server | The anomalies in the video, such as the car driving on the wrong side of the road and the car driving in the wrong direction, are likely caused by a combination of factors. First, the driver may have been under the influence of alcohol or drugs, which can impair their ability to drive safely and correctly. Second, the driver may have been distracted by something, such as a cell phone or a conversation, which led them to make a wrong turn or drive on the wrong side of the road. Third, the driver may have been inexperienced or not familiar with the area, leading them to make mistakes while navigating the road. Lastly, the driver may have been experiencing a medical emergency or a sudden loss of control, which led to the car driving in the wrong direction.

In any case, the video shows that the driver's actions were not in accordance with the rules of the road, and as a result, they caused a dangerous situation for themselves and others on the road. It is essential for drivers to be aware of their surroundings, follow traffic rules, and maintain their focus on the road to ensure the safety of themselves and others.
2024-01-30 23:01:14 | ERROR | stderr |  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 18/20 [11:17<00:59, 29.86s/it]
2024-01-30 23:01:16 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 23:01:39 | INFO | gradio_web_server | The anomalies in the video are likely caused by a combination of factors, including human error, mechanical failure, and external factors. In the video, we see a car driving on the wrong side of the road, which is a clear indication of human error. Additionally, there are multiple cars driving in the wrong direction, which could be due to a mechanical failure or a lack of proper traffic control. Furthermore, the presence of a truck driving on the wrong side of the road and a car driving on the wrong side of the road in the same scene suggests that external factors, such as road conditions or traffic congestion, might be contributing to the confusion.

To prevent such incidents in the future, it is essential to improve traffic management systems, ensure proper training for drivers, and maintain vehicles in good working condition. Additionally, drivers should be vigilant and follow traffic rules to avoid accidents and ensure the safety of all road users.
2024-01-30 23:01:39 | ERROR | stderr |  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎      | 19/20 [11:42<00:28, 28.23s/it]
2024-01-30 23:01:40 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 23:02:09 | INFO | gradio_web_server | The anomalies in the video are likely caused by a combination of factors, including human error, mechanical failure, and external factors. In the video, we see a car driving down the street and suddenly swerving to avoid a person who is standing in the middle of the road. This could be due to the driver not paying attention to the road or the person, or the person suddenly appearing in the road without warning. Additionally, the car may have experienced a mechanical failure, such as a flat tire or brake failure, which led to the driver losing control and swerving to avoid the person. Furthermore, external factors like road conditions, weather, or even the presence of animals on the road could also contribute to the driver's inability to maintain control of the vehicle. To prevent such incidents in the future, it is essential for drivers to remain vigilant and attentive to their surroundings, as well as to ensure that their vehicles are well-maintained and functioning properly.
2024-01-30 23:02:09 | ERROR | stderr | 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [12:12<00:00, 28.70s/it]
2024-01-30 23:02:09 | ERROR | stderr | 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [12:12<00:00, 36.60s/it]
2024-01-30 23:02:09 | ERROR | stderr | 
2024-01-30 23:50:00 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-30 23:50:00 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-30 23:50:01 | ERROR | stderr | Loading checkpoint shards:   0%|                                                                                                                     | 0/2 [00:00<?, ?it/s]
2024-01-30 23:50:09 | ERROR | stderr | Loading checkpoint shards:  50%|██████████████████████████████████████████████████████▌                                                      | 1/2 [00:07<00:07,  7.81s/it]
2024-01-30 23:50:13 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.48s/it]
2024-01-30 23:50:13 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.83s/it]
2024-01-30 23:50:13 | ERROR | stderr | 
2024-01-30 23:50:30 | INFO | stdout | Loading weights from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-30 23:50:31 | INFO | stdout | Weights loaded from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-30 23:50:45 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.embeddings.token_embedding.weight', 'visual_projection.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'logit_scale', 'text_projection.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-30 23:50:45 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.embeddings.token_embedding.weight', 'visual_projection.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'logit_scale', 'text_projection.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-30 23:50:46 | INFO | stdout | Initialization Finished
2024-01-30 23:50:46 | ERROR | stderr |   0%|                                                                                                                                               | 0/20 [00:00<?, ?it/s]
2024-01-30 23:50:54 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 23:50:54 | ERROR | stderr | /home/dh/anaconda3/envs/video_chatgpt/lib/python3.8/site-packages/transformers/generation/utils.py:1211: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
2024-01-30 23:50:54 | ERROR | stderr |   warnings.warn(
2024-01-30 23:51:29 | INFO | gradio_web_server | In the video, we see a car driving down a highway and suddenly swerving to avoid a large object in the road. The car then crashes into a guardrail and comes to a stop. The driver and passenger are seen exiting the car and walking away. The video also shows a person walking down the street and a car driving down the street. The car then stops and the driver gets out of the car. The video ends with the driver walking away from the car.

The anomalies in the video include the car swerving to avoid the large object in the road, the car crashing into the guardrail, and the driver and passenger exiting the car. These events have several consequences, including the potential for injury to the driver and passenger, damage to the car, and disruption to traffic flow on the highway. Additionally, the presence of the large object in the road could have been a result of an accident or a deliberate act, which could have further consequences for the surrounding environment and people.

In conclusion, the video shows a car swerving to avoid a large object in the road, which leads to a crash and the exit of the driver and passenger. The consequences of these events include potential injury to the driver and passenger, damage to the car, and disruption to traffic flow on the highway. The presence of the large object in the road could also have broader implications for the surrounding environment and people.
2024-01-30 23:51:29 | ERROR | stderr |   5%|██████▊                                                                                                                                | 1/20 [00:43<13:37, 43.05s/it]
2024-01-30 23:51:32 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 23:51:54 | INFO | gradio_web_server | In the video, there are several cars parked in the driveway, and a person is seen walking around the grassy area. The cars are parked in a way that makes it difficult for the person to walk around the grassy area. This creates an unusual situation where the person has to walk around the cars to reach the grassy area. The cars are parked in a way that obstructs the person's path, which could lead to inconvenience or even accidents if the person is not careful. The cars are parked in a way that is not typical for a driveway, which could be due to a lack of space or a temporary arrangement. The person walking around the cars might have to find an alternative route to reach the grassy area, which could be time-consuming and inconvenient. The cars parked in this manner also create a cluttered and disorganized appearance in the driveway, which might be aesthetically unpleasant for the residents.
2024-01-30 23:51:54 | ERROR | stderr |  10%|█████████████▌                                                                                                                         | 2/20 [01:07<09:43, 32.40s/it]
2024-01-30 23:51:59 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 23:55:38 | INFO | gradio_web_server | In the video, we see a school bus running red lights and causing a traffic accident. The bus driver is seen getting out of the bus and walking away. The bus is then towed away. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away from the bus and talking to the camera. The video also shows a man in a white shirt and black pants talking to the camera. The man is seen walking away
2024-01-30 23:55:38 | ERROR | stderr |  15%|████████████████████                                                                                                                  | 3/20 [04:52<34:02, 120.15s/it]
2024-01-30 23:55:41 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 23:59:15 | INFO | gradio_web_server | The video shows a car accident involving a white car and a red car. The white car is seen driving down the street and suddenly swerving to avoid the red car. The white car then crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes into a pole and comes to a stop. The driver of the white car is seen getting out of the car and walking away. The red car is also seen driving down the street, but it is unclear if it was involved in the accident or not.

The video shows a man driving a car and suddenly swerving to avoid a red car. The man's actions result in a car accident, where the white car crashes
2024-01-30 23:59:15 | ERROR | stderr |  20%|██████████████████████████▊                                                                                                           | 4/20 [08:29<42:11, 158.23s/it]
2024-01-30 23:59:29 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-30 23:59:57 | INFO | gradio_web_server | In the video, there are several instances of anomalies, such as a car driving on the wrong side of the road, a car driving on the sidewalk, and a car driving in the wrong direction. These anomalies have severe consequences, such as endangering the lives of pedestrians, other drivers, and passengers. The car driving on the wrong side of the road poses a significant risk to other drivers and pedestrians, as it is not possible to predict the car's actions or intentions. Similarly, the car driving on the sidewalk can cause accidents and injuries to pedestrians who are not expecting a vehicle to be on the sidewalk. The car driving in the wrong direction can lead to collisions with other vehicles, pedestrians, or obstacles, causing severe damage and potential fatalities.

In conclusion, these anomalies in the video highlight the importance of adhering to traffic rules and regulations to ensure the safety of all road users. The consequences of these actions can be devastating, and it is crucial to be aware of the potential risks and take necessary precautions to prevent accidents and injuries.
2024-01-30 23:59:57 | ERROR | stderr |  25%|█████████████████████████████████▌                                                                                                    | 5/20 [09:11<29:07, 116.47s/it]
2024-01-31 00:00:01 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:03:46 | INFO | gradio_web_server | In the video, we see a car driving down the road and suddenly swerving to avoid a person who is lying on the road. The car then continues driving and the person is seen lying on the road. The car then stops and the person is seen getting up and walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is seen walking away. The car then continues driving and the person is seen walking away. The car then stops again and the person is
2024-01-31 00:03:46 | ERROR | stderr |  30%|████████████████████████████████████████▏                                                                                             | 6/20 [13:00<36:04, 154.60s/it]
2024-01-31 00:03:49 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:04:29 | INFO | gradio_web_server | In the video, we see a car accident involving a police officer and a driver. The officer is seen talking to the driver, who is then pulled over by the officer. The officer then gets out of his car and approaches the driver. The driver is then seen getting out of the car and walking away. The officer then gets back into his car and drives away.

The results of this accident include the driver being pulled over by the officer, the driver getting out of the car, and the officer getting out of his car. The officer then talks to the driver, and the driver is seen walking away. The officer then gets back into his car and drives away.

The impact of this accident on the environment and people can be seen in the form of the driver being pulled over by the officer, the driver getting out of the car, and the officer getting out of his car. The officer's actions, such as talking to the driver, can also have an impact on the situation. The driver's actions, like getting out of the car, can also affect the environment and people around them. The officer's actions, like driving away, can also have an impact on the situation.

In conclusion, the accident in the video has various consequences, including the driver being pulled over, the driver getting out of the car, and the officer getting out of his car. The officer's actions, such as talking to the driver, can also have an impact on the situation. The driver's actions, like getting out of the car, can affect the environment and people around them. The officer's actions, like driving away, can also have an impact on the situation.
2024-01-31 00:04:29 | ERROR | stderr |  35%|██████████████████████████████████████████████▉                                                                                       | 7/20 [13:43<25:36, 118.22s/it]
2024-01-31 00:04:32 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:08:02 | INFO | gradio_web_server | In the video, a man is seen holding a gun and shooting at another man. The bullet hits the man in the chest, causing him to fall to the ground. The man who was shot is seen lying on the ground, bleeding from the wound. The bullet has also caused a ripple effect, as it has hit a bottle, which then shatters and breaks. The bullet has also hit a car, causing it to flip over. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a wall, causing it to crumble. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from the wound. The bullet has also hit a person, who is lying on the ground, bleeding from
2024-01-31 00:08:02 | ERROR | stderr |  40%|█████████████████████████████████████████████████████▌                                                                                | 8/20 [17:15<29:38, 148.19s/it]
2024-01-31 00:08:03 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:08:24 | INFO | gradio_web_server | In the video, we see a car crashing into a traffic light, which causes the traffic light to fall and break. This event has severe consequences for the environment, people, and the overall situation. The car crash has caused the traffic light to fall, which has led to a traffic jam and disrupted the flow of traffic. The broken traffic light has also caused confusion among drivers, leading to potential accidents and injuries. Furthermore, the fallen traffic light has damaged the surrounding infrastructure, such as the road and the traffic light itself. The car crash has also affected the environment, as the car's impact on the traffic light may have caused damage to the surrounding area. Overall, the car crash has had a significant impact on the environment, people, and the overall situation, highlighting the importance of proper traffic management and adherence to traffic rules.
2024-01-31 00:08:24 | ERROR | stderr |  45%|████████████████████████████████████████████████████████████▎                                                                         | 9/20 [17:38<19:58, 108.91s/it]
2024-01-31 00:08:27 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:08:57 | INFO | gradio_web_server | In the video, there are several instances of people engaging in unconventional activities, such as a man throwing a punch at another man, a man holding a gun, and a man jumping over a car. These anomalies have severe consequences, affecting both the individuals involved and the environment. For instance, the man throwing a punch could lead to a physical altercation, potentially resulting in injuries or even fatalities. The man holding a gun could cause a dangerous situation, as it may lead to a shooting or other violent incidents. The man jumping over a car could cause damage to the vehicle or result in injuries to the car's occupants.

These events highlight the importance of maintaining order and safety in public spaces. The presence of such anomalies in the video serves as a reminder of the need for law enforcement and community members to work together to prevent and address such incidents. Additionally, it emphasizes the importance of promoting peaceful and respectful interactions among individuals, as well as the need for effective communication and conflict resolution skills.
2024-01-31 00:08:57 | ERROR | stderr |  50%|███████████████████████████████████████████████████████████████████                                                                   | 10/20 [18:11<14:14, 85.47s/it]
2024-01-31 00:09:01 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:09:58 | INFO | gradio_web_server | In the video, a car is seen driving down the road and suddenly swerving to avoid a person who is standing in the middle of the road. The car then crashes into a guardrail and comes to a stop. The driver and the person in the middle of the road are both injured in the crash. The video shows the aftermath of the crash, with the car's front end crumpled and the guardrail bent. The person in the middle of the road is seen lying on the ground, while the driver is seen sitting in the car with a bloody face. The video ends with the car's driver being taken away in an ambulance.

The results of this anomaly include severe injuries to both the driver and the person in the middle of the road. The person in the middle of the road is seen lying on the ground, which indicates that they were not able to move or avoid the car's path. The driver's injuries are more severe, as they are seen with a bloody face, which suggests that they were more directly involved in the crash. The crash also caused damage to the car's front end and the guardrail, which could lead to further complications or expenses for the driver and the car owner.

The impact of this event on the environment and people is significant. The person in the middle of the road is likely to suffer from severe injuries, which could lead to long-term health issues or even death. The driver's injuries are also severe, which could result in hospitalization, rehabilitation, and potential loss of income due to time off work. The crash itself has caused damage to the car and the guardrail, which could lead to additional expenses for repairs or replacement. Furthermore, the crash could also cause traffic disruptions, as the road may need to be closed for investigation or cleanup. Overall, the event has had a significant impact on the people involved and the environment, highlighting the importance of road safety and the need for proper infrastructure to prevent such incidents.
2024-01-31 00:09:58 | ERROR | stderr |  55%|█████████████████████████████████████████████████████████████████████████▋                                                            | 11/20 [19:12<11:42, 78.04s/it]
2024-01-31 00:10:00 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:10:33 | INFO | gradio_web_server | In the video, we see a man riding a motorcycle on a road, and the camera captures the view of the road from the motorcycle's perspective. Suddenly, the motorcycle veers off the road and crashes into a tree. The man is thrown off the motorcycle and is seen lying on the ground. The video then shows the aftermath of the crash, with the motorcycle lying on the ground and the man lying on the road.

The crash has several consequences, including the man's injury, the damage to the motorcycle, and the potential impact on the environment. The man's injury could range from minor bruises to severe injuries, depending on the extent of the crash and the impact of the motorcycle on him. The motorcycle's damage could range from minor scratches to severe damage, depending on the angle of the crash and the force of the impact. The crash could also have an impact on the environment, such as causing debris to scatter across the road or damaging nearby vegetation.

In conclusion, the crash of the motorcycle on the road has several consequences, including the injury of the rider, the damage to the motorcycle, and the potential impact on the environment. The severity of these consequences depends on the extent of the crash and the impact of the motorcycle on the rider and the surroundings.
2024-01-31 00:10:33 | ERROR | stderr |  60%|████████████████████████████████████████████████████████████████████████████████▍                                                     | 12/20 [19:47<08:39, 64.96s/it]
2024-01-31 00:10:35 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:10:58 | INFO | gradio_web_server | In the video, a man is seen walking down a street and suddenly a car appears out of nowhere and crashes into him. The man falls to the ground, and the car continues to drive away. This event has severe consequences for the man, as he is left lying on the ground with a broken leg. The car's driver, who is responsible for the accident, is also affected by the incident. The driver's actions have resulted in a severe injury to the pedestrian, which could lead to legal consequences, financial burden, and emotional distress for both parties involved. Additionally, the car's driver may face penalties, fines, or even imprisonment depending on the severity of the accident and the legal system in the country. The incident also highlights the importance of being cautious and attentive while driving, as even a momentary lapse in attention can lead to severe consequences for both the driver and the pedestrian.
2024-01-31 00:10:58 | ERROR | stderr |  65%|███████████████████████████████████████████████████████████████████████████████████████                                               | 13/20 [20:12<06:08, 52.69s/it]
2024-01-31 00:11:04 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:11:36 | INFO | gradio_web_server | In the video, we see a car driving down the street and suddenly swerving to avoid a pedestrian. The car then crashes into a parked car, causing it to flip over. The driver and passenger are seen getting out of the car and walking away. The video also shows a person walking down the street and a car driving down the street. The car then stops and the driver gets out of the car. The video ends with the car driving away.

The results of these anomalies are severe. The car crash caused the car to flip over, which could have resulted in severe injuries or even fatalities for the driver and passenger. The pedestrian who was swerved at could have been injured or even killed. The driver and passenger of the car that crashed are seen walking away, which suggests that they might have escaped the crash with minor injuries. However, the overall impact of the crash on the environment, people, and the overall situation is not clear from the video alone.

The video highlights the importance of being cautious and attentive while driving, as even a momentary lapse in attention can lead to severe consequences. It also emphasizes the need for proper road safety measures and the importance of following traffic rules to minimize the risk of accidents.
2024-01-31 00:11:36 | ERROR | stderr |  70%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                        | 14/20 [20:50<04:49, 48.25s/it]
2024-01-31 00:11:37 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:15:39 | INFO | gradio_web_server | In the video, a man is seen driving a car down a street and suddenly swerving to the right. The car then crashes into a tree and explodes. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the street and suddenly swerving to the right, causing it to crash into a tree. The driver is seen getting out of the car and walking away. The video also shows a car driving down the
2024-01-31 00:15:39 | ERROR | stderr |  75%|███████████████████████████████████████████████████████████████████████████████████████████████████▊                                 | 15/20 [24:53<08:55, 107.14s/it]
2024-01-31 00:15:44 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:16:09 | INFO | gradio_web_server | In the video, there are several instances of traffic lights malfunctioning, causing cars to stop and go in the wrong direction. This leads to a chaotic traffic situation, with cars driving in the wrong direction and causing accidents. The video shows a car driving in the wrong direction and colliding with another car, resulting in a fire. The fire spreads to nearby buildings, causing significant damage to the structures. Additionally, the video shows a person running in the street, which could be a result of the chaos caused by the malfunctioning traffic lights. The person is seen running towards a fire hydrant, which could be a result of the fire spreading to nearby buildings. The video ends with the fire hydrant being opened, and the fire being extinguished. The malfunctioning traffic lights and the person running in the street are directly linked to the fire and the damage caused to the buildings. The chaos caused by the malfunctioning traffic lights led to the wrong direction of the cars, which ultimately caused the fire and the damage to the buildings.
2024-01-31 00:16:09 | ERROR | stderr |  80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▏                          | 16/20 [25:23<05:35, 83.93s/it]
2024-01-31 00:16:18 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:16:43 | INFO | gradio_web_server | In the video, there are several instances of traffic violations, such as speeding, tailgating, and lane changing without signaling. These violations can lead to severe consequences, including accidents, injuries, and fatalities. The video shows a car driving at high speed and swerving to avoid a truck, which could have resulted in a collision if the driver had not taken evasive action. Additionally, the video shows a car driving on the shoulder of the road, which is a dangerous maneuver that can lead to accidents and endanger other road users.

These anomalies in the video highlight the importance of adhering to traffic rules and maintaining safe driving practices. By following traffic regulations, drivers can minimize the risk of accidents and ensure the safety of themselves and others on the road. The video serves as a reminder of the importance of being attentive and responsible while driving, and the consequences of not doing so.
2024-01-31 00:16:43 | ERROR | stderr |  85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                    | 17/20 [25:57<03:26, 68.93s/it]
2024-01-31 00:16:44 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:17:13 | INFO | gradio_web_server | In the video, we see a car driving down the street and suddenly swerving into the opposite lane. This is an unusual event, as it is not common for a driver to change lanes without signaling or checking for oncoming traffic. The car then collides with another vehicle, causing it to spin out of control. The driver of the car is seen getting out of the vehicle and walking away.

The impact of this event on the environment and people can be severe. The collision between the two vehicles has caused significant damage to both cars, and it is likely that the driver of the car that swerved into the opposite lane will face legal consequences for their actions. Additionally, the spinning out of control of the vehicle could have caused injury to the driver or other road users. The incident also highlights the importance of following traffic rules and being aware of one's surroundings while driving.

In conclusion, the video shows a car swerving into the opposite lane and colliding with another vehicle, resulting in severe consequences for the environment, people, and the driver involved. The incident serves as a reminder of the importance of following traffic rules and being vigilant while driving.
2024-01-31 00:17:13 | ERROR | stderr |  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 18/20 [26:27<01:54, 57.17s/it]
2024-01-31 00:17:15 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:17:43 | INFO | gradio_web_server | In the video, there are several instances of traffic violations, such as cars driving on the wrong side of the road, cars driving in the wrong direction, and cars driving at high speeds. These anomalies have severe consequences, such as accidents, injuries, and fatalities. The video shows a car driving on the wrong side of the road and colliding with another car, resulting in a severe accident. The driver of the car driving on the wrong side of the road is at fault for the accident, and the consequences of this accident can be devastating for the people involved.

In another instance, a car is seen driving in the wrong direction on a one-way street, which can lead to accidents and injuries. The driver of the car is at fault for this violation, and the consequences of this action can be severe for the driver and other road users.

These traffic violations highlight the importance of adhering to traffic rules and regulations to ensure the safety of all road users. The video serves as a reminder of the potential dangers of reckless driving and the need for drivers to be cautious and attentive to their surroundings.
2024-01-31 00:17:43 | ERROR | stderr |  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎      | 19/20 [26:57<00:48, 48.95s/it]
2024-01-31 00:17:45 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:18:09 | INFO | gradio_web_server | In the video, there are several instances of abnormal events, such as a car driving on the wrong side of the road, a person walking on the road, and a person standing in the middle of the road. These events have severe consequences, such as endangering the lives of other drivers and pedestrians, causing traffic jams, and potentially leading to accidents. The car driving on the wrong side of the road is particularly dangerous, as it puts the driver and other road users at risk. The person walking on the road also poses a risk to both the pedestrian and the drivers, as they may not be visible to the drivers and could suddenly step into the path of an oncoming vehicle. The person standing in the middle of the road is also a hazard, as they may not be aware of the traffic flow and could cause accidents if they do not move quickly. These events highlight the importance of adhering to traffic rules and being aware of one's surroundings to ensure the safety of all road users.
2024-01-31 00:18:09 | ERROR | stderr | 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [27:23<00:00, 42.06s/it]
2024-01-31 00:18:09 | ERROR | stderr | 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [27:23<00:00, 82.17s/it]
2024-01-31 00:18:09 | ERROR | stderr | 
2024-01-31 00:38:16 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-31 00:38:16 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-31 00:38:17 | ERROR | stderr | Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                             | 0/2 [00:00<?, ?it/s]
2024-01-31 00:38:24 | ERROR | stderr | Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                          | 1/2 [00:07<00:07,  7.76s/it]
2024-01-31 00:38:27 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  4.89s/it]
2024-01-31 00:38:27 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.32s/it]
2024-01-31 00:38:27 | ERROR | stderr | 
2024-01-31 00:38:45 | INFO | stdout | Loading weights from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-31 00:38:45 | INFO | stdout | Weights loaded from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-31 00:38:57 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'logit_scale', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-31 00:38:57 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'logit_scale', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-31 00:38:57 | INFO | stdout | Initialization Finished
2024-01-31 00:38:57 | ERROR | stderr |   0%|                                                                                                                                                                                                                                                                                       | 0/20 [00:00<?, ?it/s]
2024-01-31 00:39:05 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:39:05 | ERROR | stderr | /home/dh/anaconda3/envs/video_chatgpt/lib/python3.8/site-packages/transformers/generation/utils.py:1211: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
2024-01-31 00:39:05 | ERROR | stderr |   warnings.warn(
2024-01-31 00:39:10 | INFO | gradio_web_server | Yes, there is a man in the video who is seen driving a car and talking on his phone. This is considered an unusual event because it is not common for people to talk on their phones while driving.
2024-01-31 00:39:10 | ERROR | stderr |   5%|█████████████▌                                                                                                                                                                                                                                                                 | 1/20 [00:12<04:04, 12.87s/it]
2024-01-31 00:39:13 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:39:23 | INFO | gradio_web_server | Yes, there is an unusual event in the video. A man is seen lying on the grass in the middle of the road, while cars are driving around him. This is not a typical scene, as people usually avoid lying on the road to avoid accidents or disrupting traffic. The presence of the man lying on the grass in the middle of the road adds an element of surprise and unpredictability to the video.
2024-01-31 00:39:23 | ERROR | stderr |  10%|███████████████████████████                                                                                                                                                                                                                                                    | 2/20 [00:25<03:49, 12.77s/it]
2024-01-31 00:39:28 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:43:03 | INFO | gradio_web_server | Yes, there is a man in the video who is seen driving a school bus and then getting out of it. He is then seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt who is seen walking down the street and talking to a woman. The video also shows a man in a red shirt
2024-01-31 00:43:03 | ERROR | stderr |  15%|████████████████████████████████████████▌                                                                                                                                                                                                                                     | 3/20 [04:05<30:24, 107.34s/it]
2024-01-31 00:43:05 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:43:11 | INFO | gradio_web_server | Yes, there is a man in a white shirt who is seen driving a car and then getting out of it. He is then seen walking down the street and talking to a woman. The video also shows a car accident involving a white car and a red car.
2024-01-31 00:43:11 | ERROR | stderr |  20%|██████████████████████████████████████████████████████▏                                                                                                                                                                                                                        | 4/20 [04:14<18:15, 68.44s/it]
2024-01-31 00:43:25 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:43:31 | INFO | gradio_web_server | Yes, there is a man in the video who is seen driving a car and then suddenly stops and gets out of the car. This is unusual as it is not common for a driver to stop in the middle of the road and get out of their car.
2024-01-31 00:43:31 | ERROR | stderr |  25%|███████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                           | 5/20 [04:33<12:40, 50.72s/it]
2024-01-31 00:43:35 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:43:39 | INFO | gradio_web_server | Yes, there is a man in the video who is seen driving a car and then getting out of it. He is then seen walking towards the camera and talking to the person behind it.
2024-01-31 00:43:39 | ERROR | stderr |  30%|█████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                             | 6/20 [04:42<08:29, 36.37s/it]
2024-01-31 00:43:43 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:43:49 | INFO | gradio_web_server | Yes, there is a man in the video who is seen driving a car and then getting out of it. He is then seen talking to a police officer and pointing at the camera. The police officer is seen talking to the man and then driving away.
2024-01-31 00:43:49 | ERROR | stderr |  35%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                | 7/20 [04:51<05:58, 27.54s/it]
2024-01-31 00:43:52 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:43:57 | INFO | gradio_web_server | Yes, there is an unusual event in the video. A man is seen holding a gun and pointing it at another man. This is not a typical scene in a video, as guns are often associated with violence and crime.
2024-01-31 00:43:57 | ERROR | stderr |  40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                  | 8/20 [04:59<04:17, 21.46s/it]
2024-01-31 00:43:59 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:44:01 | INFO | gradio_web_server | Yes, there is a car accident in the video, which is an unusual event.
2024-01-31 00:44:01 | ERROR | stderr |  45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                     | 9/20 [05:03<02:55, 15.98s/it]
2024-01-31 00:44:03 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:44:13 | INFO | gradio_web_server | Yes, there is an unusual event in the video. A man is seen getting into a car and driving away, while another man is seen getting out of the car and walking away. This is unusual because the man who got out of the car is not seen driving away, and the man who got into the car is not seen walking away. This creates a sense of confusion and uncertainty about the events that took place.
2024-01-31 00:44:13 | ERROR | stderr |  50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                       | 10/20 [05:15<02:27, 14.75s/it]
2024-01-31 00:44:16 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:44:21 | INFO | gradio_web_server | Yes, there is a man in the video who is driving a car and talking on his phone. This is unusual because it is generally not safe to use a phone while driving.
2024-01-31 00:44:21 | ERROR | stderr |  55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                         | 11/20 [05:23<01:53, 12.59s/it]
2024-01-31 00:44:23 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:44:27 | INFO | gradio_web_server | Yes, there is a man in the video who is seen riding a motorcycle and talking to the camera. He is also seen riding the motorcycle on the road and talking to the camera.
2024-01-31 00:44:27 | ERROR | stderr |  60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                            | 12/20 [05:30<01:26, 10.82s/it]
2024-01-31 00:44:29 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:44:35 | INFO | gradio_web_server | Yes, there is an unusual event in the video. A man is seen walking down the street with a gun in his hand. This is not a typical scene in a city, as it is not common to see people walking around with guns in public.
2024-01-31 00:44:35 | ERROR | stderr |  65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                              | 13/20 [05:37<01:09,  9.88s/it]
2024-01-31 00:44:42 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:44:47 | INFO | gradio_web_server | Yes, there is a man in the video who is driving a car and talking on his phone. This is considered an unusual event because it is not common for people to talk on their phones while driving.
2024-01-31 00:44:47 | ERROR | stderr |  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                 | 14/20 [05:50<01:03, 10.58s/it]
2024-01-31 00:44:48 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:44:50 | INFO | gradio_web_server | Yes, there is a man in a car who is wrong-way driving in Florida.
2024-01-31 00:44:50 | ERROR | stderr |  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                   | 15/20 [05:53<00:41,  8.28s/it]
2024-01-31 00:44:55 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:45:00 | INFO | gradio_web_server | Yes, there is a man in the video who is seen walking down the street and then suddenly collapses. This is an unusual event as it is not common for people to collapse on the street without any apparent reason.
2024-01-31 00:45:00 | ERROR | stderr |  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                      | 16/20 [06:03<00:35,  8.83s/it]
2024-01-31 00:45:08 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:45:16 | INFO | gradio_web_server | Yes, there is a man in the video who is seen driving a car and then getting out of it. He is seen walking towards the camera and then getting back into the car. This is an unusual event as it is not common for a person to get out of a car and walk towards the camera while the car is still running.
2024-01-31 00:45:16 | ERROR | stderr |  85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                        | 17/20 [06:18<00:32, 10.90s/it]
2024-01-31 00:45:17 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:45:22 | INFO | gradio_web_server | Yes, there is a man in the video who is seen running across the street and getting hit by a car. This is an unusual event as it is not common for a person to run across the street and get hit by a car.
2024-01-31 00:45:22 | ERROR | stderr |  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                           | 18/20 [06:25<00:19,  9.53s/it]
2024-01-31 00:45:24 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:45:25 | INFO | gradio_web_server | No, there is no unusual event in the video.
2024-01-31 00:45:26 | ERROR | stderr |  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 19/20 [06:28<00:07,  7.60s/it]
2024-01-31 00:45:27 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:45:29 | INFO | gradio_web_server | No, there is no unusual event in the video.
2024-01-31 00:45:29 | ERROR | stderr | 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [06:31<00:00,  6.26s/it]
2024-01-31 00:45:29 | ERROR | stderr | 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [06:31<00:00, 19.58s/it]
2024-01-31 00:45:29 | ERROR | stderr | 
2024-01-31 00:46:59 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-31 00:46:59 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-31 00:47:00 | ERROR | stderr | Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                             | 0/2 [00:00<?, ?it/s]
2024-01-31 00:47:08 | ERROR | stderr | Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                          | 1/2 [00:07<00:07,  7.88s/it]
2024-01-31 00:47:11 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  4.91s/it]
2024-01-31 00:47:11 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.36s/it]
2024-01-31 00:47:11 | ERROR | stderr | 
2024-01-31 00:47:28 | INFO | stdout | Loading weights from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-31 00:47:28 | INFO | stdout | Weights loaded from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-31 00:47:51 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'logit_scale', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'visual_projection.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-31 00:47:51 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'logit_scale', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'visual_projection.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-31 00:47:51 | INFO | stdout | Initialization Finished
2024-01-31 00:47:51 | ERROR | stderr |   0%|                                                                                                                                                                                                                                                                                       | 0/20 [00:00<?, ?it/s]
2024-01-31 00:47:59 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:47:59 | ERROR | stderr | /home/dh/anaconda3/envs/video_chatgpt/lib/python3.8/site-packages/transformers/generation/utils.py:1211: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
2024-01-31 00:47:59 | ERROR | stderr |   warnings.warn(
2024-01-31 00:48:05 | INFO | gradio_web_server | The video shows a man driving a car and talking on his phone. The car is seen driving down the road and the man is seen talking on his phone. The video also shows a man talking on his phone while driving a car.
2024-01-31 00:48:05 | ERROR | stderr |   5%|█████████████▌                                                                                                                                                                                                                                                                 | 1/20 [00:13<04:24, 13.93s/it]
2024-01-31 00:48:08 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:48:17 | INFO | gradio_web_server | In the video, a man is seen lying on the grass in the middle of a circle of cars. This is an unusual event, as it is not common for people to lie down in the middle of a circle of parked cars. The man is lying on his back, and there are cars parked around him. This scene is categorized as a "Pedestrian Incident" under the "Forbidden to Burn" category.
2024-01-31 00:48:17 | ERROR | stderr |  10%|███████████████████████████                                                                                                                                                                                                                                                    | 2/20 [00:26<03:55, 13.10s/it]
2024-01-31 00:48:22 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:51:47 | INFO | gradio_web_server | After watching the video, I observed a man driving a school bus and a woman driving a red car. The man is driving the school bus and the woman is driving the red car. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt. The video also shows a man in a red shirt and a woman in a white shirt
2024-01-31 00:51:47 | ERROR | stderr |  15%|████████████████████████████████████████▌                                                                                                                                                                                                                                     | 3/20 [03:55<29:05, 102.68s/it]
2024-01-31 00:51:49 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:55:30 | INFO | gradio_web_server | The video shows a man driving a car and a woman in a white shirt and black pants standing in the street. The man is driving the car and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and the woman is standing in the street. The car is driving down the street and
2024-01-31 00:55:30 | ERROR | stderr |  20%|██████████████████████████████████████████████████████                                                                                                                                                                                                                        | 4/20 [07:38<40:03, 150.21s/it]
2024-01-31 00:55:43 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:56:03 | INFO | gradio_web_server | 1. Fighting: There is a man fighting another man in the middle of the street.
2. Animals Hurting People: A dog is chasing a man in the street.
3. Water Incidents: A car is driving through a flooded street.
4. Vandalism: A man is seen vandalizing a car.
5. Traffic Accidents: A car is seen crashing into a wall.
6. Robbery: A man is seen stealing a car.
7. Theft: A man is seen stealing a wallet from a car.
8. Pedestrian Incidents: A man is seen running into the street and getting hit by a car.
9. Fire: A car is seen on fire.
10. Traffic Violations: A car is seen driving the wrong way on the street.
11. Forbidden to Burn: A man is seen burning a car.
2024-01-31 00:56:03 | ERROR | stderr |  25%|███████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                          | 5/20 [08:12<27:01, 108.08s/it]
2024-01-31 00:56:07 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:56:10 | INFO | gradio_web_server | The video shows a man driving a car and then getting out of it. The car is then shown driving away.
2024-01-31 00:56:10 | ERROR | stderr |  30%|█████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                             | 6/20 [08:18<17:10, 73.64s/it]
2024-01-31 00:56:13 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:56:18 | INFO | gradio_web_server | The video shows a man driving a car and talking on his phone. The car then crashes into a tree and the man gets out of the car. The man then gets into a police car and the police car drives away.
2024-01-31 00:56:18 | ERROR | stderr |  35%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                | 7/20 [08:27<11:20, 52.35s/it]
2024-01-31 00:56:22 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 00:56:25 | INFO | gradio_web_server | In the video, a man is seen punching another man in the face. This event falls under the category of fighting.
2024-01-31 00:56:25 | ERROR | stderr |  40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                  | 8/20 [08:33<07:32, 37.67s/it]
2024-01-31 00:56:26 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:00:01 | INFO | gradio_web_server | In the video, a man is seen driving a car and then crashing into a pole. The car is then seen on fire. The video also shows a man being hit by a car. Additionally, there is a scene of a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man being hit by a car. The video also shows a man
2024-01-31 01:00:01 | ERROR | stderr |  45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                     | 9/20 [12:10<17:09, 93.59s/it]
2024-01-31 01:00:04 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:00:07 | INFO | gradio_web_server | In the video, a man is seen punching another man in the face. This event falls under the category of fighting.
2024-01-31 01:00:07 | ERROR | stderr |  50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                       | 10/20 [12:15<11:04, 66.44s/it]
2024-01-31 01:00:10 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:00:13 | INFO | gradio_web_server | The video shows a man driving a car and then getting out of the car and walking away. The car then drives away.
2024-01-31 01:00:13 | ERROR | stderr |  55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                         | 11/20 [12:22<07:12, 48.06s/it]
2024-01-31 01:00:15 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:03:50 | ERROR | stderr |  55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                         | 11/20 [15:59<13:04, 87.21s/it]
2024-01-31 01:03:50 | ERROR | stderr | 
2024-01-31 01:03:50 | ERROR | stderr | Traceback (most recent call last):
2024-01-31 01:03:50 | ERROR | stderr |   File "CUVA_demo.py", line 199, in <module>
2024-01-31 01:03:50 | ERROR | stderr |     score = CUVA(chat_map['default'],video_path, prompt, mod = args.mod)
2024-01-31 01:03:50 | ERROR | stderr |   File "CUVA_demo.py", line 92, in CUVA
2024-01-31 01:03:50 | ERROR | stderr |     for result in result_generator:
2024-01-31 01:03:50 | ERROR | stderr |   File "/home/dh/pythonProject/AnomalyDataset/Video-ChatGPT/video_chatgpt/demo/chat.py", line 153, in answer
2024-01-31 01:03:50 | ERROR | stderr |     time.sleep(0.01)
2024-01-31 01:03:50 | ERROR | stderr | KeyboardInterrupt
2024-01-31 01:04:11 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-31 01:04:11 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
2024-01-31 01:04:12 | ERROR | stderr | Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                             | 0/2 [00:00<?, ?it/s]
2024-01-31 01:04:20 | ERROR | stderr | Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                          | 1/2 [00:08<00:08,  8.06s/it]
2024-01-31 01:04:23 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.03s/it]
2024-01-31 01:04:23 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.49s/it]
2024-01-31 01:04:23 | ERROR | stderr | 
2024-01-31 01:04:39 | INFO | stdout | Loading weights from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-31 01:04:40 | INFO | stdout | Weights loaded from /home/dh/zsc/VideoBench/model/Video-ChatGPT/video_chatgpt-7B.bin
2024-01-31 01:04:51 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'visual_projection.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'logit_scale', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-31 01:04:51 | WARNING | transformers.modeling_utils | Some weights of the model checkpoint at /home/disk1/cuva/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'visual_projection.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'logit_scale', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-01-31 01:04:52 | INFO | stdout | Initialization Finished
2024-01-31 01:04:52 | ERROR | stderr |   0%|                                                                                                                                                                                                                                                                                       | 0/20 [00:00<?, ?it/s]
2024-01-31 01:05:00 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:05:00 | ERROR | stderr | /home/dh/anaconda3/envs/video_chatgpt/lib/python3.8/site-packages/transformers/generation/utils.py:1211: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
2024-01-31 01:05:00 | ERROR | stderr |   warnings.warn(
2024-01-31 01:05:03 | INFO | gradio_web_server | The anomalous segment in the video is the part where the woman is talking about the traffic light.
2024-01-31 01:05:03 | ERROR | stderr |   5%|█████████████▌                                                                                                                                                                                                                                                                 | 1/20 [00:11<03:35, 11.36s/it]
2024-01-31 01:05:06 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:05:09 | INFO | gradio_web_server | The anomalous segment in the video is the part where the camera is looking up at the sky.
2024-01-31 01:05:09 | ERROR | stderr |  10%|███████████████████████████                                                                                                                                                                                                                                                    | 2/20 [00:16<02:22,  7.91s/it]
2024-01-31 01:05:13 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:05:16 | INFO | gradio_web_server | The anomalous segment in the video is the red car that is driving down the street.
2024-01-31 01:05:16 | ERROR | stderr |  15%|████████████████████████████████████████▋                                                                                                                                                                                                                                      | 3/20 [00:23<02:07,  7.52s/it]
2024-01-31 01:05:18 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:05:21 | INFO | gradio_web_server | The anomalous segment in the video is the part where the man in the white shirt is talking to the camera.
2024-01-31 01:05:21 | ERROR | stderr |  20%|██████████████████████████████████████████████████████▏                                                                                                                                                                                                                        | 4/20 [00:29<01:47,  6.72s/it]
2024-01-31 01:05:35 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:05:38 | INFO | gradio_web_server | The anomalous segment in the video is the one where the car is driving down the road and the speedometer is showing 100 mph.
2024-01-31 01:05:38 | ERROR | stderr |  25%|███████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                           | 5/20 [00:46<02:37, 10.51s/it]
2024-01-31 01:05:42 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:05:45 | INFO | gradio_web_server | The anomalous segment in the video is the part where the car is shown driving down the road.
2024-01-31 01:05:45 | ERROR | stderr |  30%|█████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                             | 6/20 [00:53<02:07,  9.14s/it]
2024-01-31 01:05:48 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:05:50 | INFO | gradio_web_server | The anomalous segment in the video is the part where the man is talking to the camera.
2024-01-31 01:05:50 | ERROR | stderr |  35%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                | 7/20 [00:58<01:43,  7.97s/it]
2024-01-31 01:05:53 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:05:56 | INFO | gradio_web_server | The anomalous segment in the video is at 0:00:00.000.
2024-01-31 01:05:56 | ERROR | stderr |  40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                  | 8/20 [01:03<01:24,  7.08s/it]
2024-01-31 01:05:57 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:06:02 | INFO | gradio_web_server | The anomalous segment in the video is the part where the news reporter is talking about the accident and the police officer is talking about the accident.
2024-01-31 01:06:02 | ERROR | stderr |  45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                     | 9/20 [01:10<01:14,  6.80s/it]
2024-01-31 01:06:04 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:06:08 | INFO | gradio_web_server | The anomalous segment in the video is at 0:00:00.000, where the man is seen holding a gun and pointing it at the camera.
2024-01-31 01:06:08 | ERROR | stderr |  50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                       | 10/20 [01:16<01:06,  6.67s/it]
2024-01-31 01:06:11 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:06:14 | INFO | gradio_web_server | The anomalous segment in the video is the part where the car is driving down the road and suddenly stops.
2024-01-31 01:06:14 | ERROR | stderr |  55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                         | 11/20 [01:22<00:58,  6.52s/it]
2024-01-31 01:06:17 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:06:19 | INFO | gradio_web_server | The anomalous segment in the video is the part where the camera is shaking.
2024-01-31 01:06:19 | ERROR | stderr |  60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                            | 12/20 [01:27<00:47,  5.92s/it]
2024-01-31 01:06:21 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:06:23 | INFO | gradio_web_server | The anomalous segment in the video is at 1:00.
2024-01-31 01:06:23 | ERROR | stderr |  65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                              | 13/20 [01:31<00:37,  5.31s/it]
2024-01-31 01:06:30 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:06:38 | INFO | gradio_web_server | The anomalous segment in the video is the part where the car is driving down the street and the camera is focused on the driver. The video is then cut to a close-up of the driver's face, and the camera is then focused on the car's dashboard. The video then cuts back to the street, and the car is seen driving down the street.
2024-01-31 01:06:38 | ERROR | stderr |  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                 | 14/20 [01:46<00:50,  8.45s/it]
2024-01-31 01:06:39 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:06:42 | INFO | gradio_web_server | The anomalous segment in the video is the presence of a car driving on the wrong side of the road.
2024-01-31 01:06:42 | ERROR | stderr |  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                   | 15/20 [01:50<00:34,  6.97s/it]
2024-01-31 01:06:47 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:06:49 | INFO | gradio_web_server | The anomalous segment in the video is the red car driving down the street.
2024-01-31 01:06:49 | ERROR | stderr |  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                      | 16/20 [01:57<00:27,  6.96s/it]
2024-01-31 01:06:57 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:07:01 | INFO | gradio_web_server | The anomalous segment in the video is the part where the truck driver is seen driving down the road with the camera mounted on the front of the truck.
2024-01-31 01:07:01 | ERROR | stderr |  85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                        | 17/20 [02:09<00:25,  8.57s/it]
2024-01-31 01:07:02 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:07:05 | INFO | gradio_web_server | The anomalous segment in the video is the white car that is driving on the wrong side of the road.
2024-01-31 01:07:05 | ERROR | stderr |  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                           | 18/20 [02:13<00:14,  7.04s/it]
2024-01-31 01:07:06 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:07:09 | INFO | gradio_web_server | The anomalous segment in the video is the part where the car is driving down the street and suddenly stops.
2024-01-31 01:07:09 | ERROR | stderr |  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 19/20 [02:17<00:06,  6.25s/it]
2024-01-31 01:07:11 | INFO | stdout | video_features shape is torch.Size([356, 1024])
2024-01-31 01:07:14 | INFO | gradio_web_server | The anomalous segment in the video is the part where the car is driving down the street and the camera is pointing at the car.
2024-01-31 01:07:14 | ERROR | stderr | 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [02:22<00:00,  5.86s/it]
2024-01-31 01:07:14 | ERROR | stderr | 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [02:22<00:00,  7.12s/it]
2024-01-31 01:07:14 | ERROR | stderr | 
